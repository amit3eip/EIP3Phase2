{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eip_Phase_2_ Assignment2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "boTqjD3P6qlt",
        "colab_type": "code",
        "outputId": "d12c82fa-4b0a-4d21-ca93-d890873816d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJd1Ydi3YcoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee36ab98-e8a7-4011-e9a5-103c690c892c"
      },
      "source": [
        "ls drive/'My Drive'/wonderland.txt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'drive/My Drive/wonderland.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwMwmv2j62Jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp drive/'My Drive'/wonderland.txt ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHeC2ASy7uOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68KjSipm7-l7",
        "colab_type": "code",
        "outputId": "1659849d-bd3d-43ec-fe03-b77f8224fc48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "raw_text[:1000]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"project gutenberg's alice's adventures in wonderland, by lewis carroll\\n\\nthis ebook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.  you may copy it, give it away or\\nre-use it under the terms of the project gutenberg license included\\nwith this ebook or online at www.gutenberg.org\\n\\n\\ntitle: alice's adventures in wonderland\\n\\nauthor: lewis carroll\\n\\nposting date: june 25, 2008 [ebook #11]\\nrelease date: march, 1994\\n[last updated: december 20, 2011]\\n\\nlanguage: english\\n\\n\\n*** start of this project gutenberg ebook alice's adventures in wonderland ***\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nalice's adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i. down the rabbit-hole\\n\\nalice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought alice 'without picture\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rVNZgn18D5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "raw_text = re.sub(r'[^\\w\\s\\t\\n]','',raw_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0Q-DV3J8PRE",
        "colab_type": "code",
        "outputId": "d2a34ee0-a212-4426-a2af-dc45adb3a67f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "chars"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wynE3v8l6dLN",
        "colab_type": "code",
        "outputId": "0da48bce-197a-4e88-f813-28c6e75c2dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  154863\n",
            "Total Vocab:  39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c1IVYOb90nA",
        "colab_type": "code",
        "outputId": "0a547d56-50dc-4b30-f200-92b04da833ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sentence = raw_text.split(\"\\n\\n\")\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "sent = ([x for x in sentence if sum(c.isalpha() for c in x)>1])\n",
        "dataX = []\n",
        "dataY = []\n",
        "seq_length = 101\n",
        "for s in sent:\n",
        "  if len(s)<101:\n",
        "    x = [char_to_int[a] for a in s]\n",
        "    x = pad_sequences([x], maxlen=seq_length)\n",
        "    dataX.append([a for a in x[0][:-1]])\n",
        "    dataY.append(x[0][-1])\n",
        "  else:\n",
        "    for i in range(0, len(s) - seq_length, 1):\n",
        "      x = [char_to_int[a] for a in s[i:i + seq_length]] \n",
        "      dataX.append([a for a in x[:-1]])\n",
        "      dataY.append(x[-1])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "\n",
        "print(len(dataX))\n",
        "X = numpy.reshape(numpy.array(dataX), (n_patterns, seq_length-1, 1))\n",
        "\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "y = np_utils.to_categorical(dataY)      "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  82268\n",
            "82268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGeEgohz8iEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "filepath=\"Eip_Phase_2_ Assignment2-amit.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aJMW2sh8kqE",
        "colab_type": "code",
        "outputId": "328341d7-72a3-414f-b49b-f04f0d50553e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=4096, callbacks=callbacks_list)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "82268/82268 [==============================] - 35s 422us/step - loss: 2.8664\n",
            "\n",
            "Epoch 00001: loss improved from 2.88216 to 2.86638, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 2/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.8583\n",
            "\n",
            "Epoch 00002: loss improved from 2.86638 to 2.85827, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 3/100\n",
            "82268/82268 [==============================] - 32s 388us/step - loss: 2.8502\n",
            "\n",
            "Epoch 00003: loss improved from 2.85827 to 2.85022, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 4/100\n",
            "82268/82268 [==============================] - 32s 388us/step - loss: 2.8401\n",
            "\n",
            "Epoch 00004: loss improved from 2.85022 to 2.84006, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 5/100\n",
            "82268/82268 [==============================] - 32s 388us/step - loss: 2.8282\n",
            "\n",
            "Epoch 00005: loss improved from 2.84006 to 2.82817, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 6/100\n",
            "82268/82268 [==============================] - 32s 388us/step - loss: 2.8182\n",
            "\n",
            "Epoch 00006: loss improved from 2.82817 to 2.81818, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 7/100\n",
            "82268/82268 [==============================] - 32s 388us/step - loss: 2.8049\n",
            "\n",
            "Epoch 00007: loss improved from 2.81818 to 2.80486, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 8/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.7986\n",
            "\n",
            "Epoch 00008: loss improved from 2.80486 to 2.79855, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 9/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.7977\n",
            "\n",
            "Epoch 00009: loss improved from 2.79855 to 2.79773, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 10/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.7817\n",
            "\n",
            "Epoch 00010: loss improved from 2.79773 to 2.78166, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 11/100\n",
            "82268/82268 [==============================] - 32s 388us/step - loss: 2.7703\n",
            "\n",
            "Epoch 00011: loss improved from 2.78166 to 2.77030, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 12/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.7585\n",
            "\n",
            "Epoch 00012: loss improved from 2.77030 to 2.75854, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 13/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.7434\n",
            "\n",
            "Epoch 00013: loss improved from 2.75854 to 2.74343, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 14/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.7329\n",
            "\n",
            "Epoch 00014: loss improved from 2.74343 to 2.73288, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 15/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.7235\n",
            "\n",
            "Epoch 00015: loss improved from 2.73288 to 2.72354, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 16/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.7162\n",
            "\n",
            "Epoch 00016: loss improved from 2.72354 to 2.71625, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 17/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.7068\n",
            "\n",
            "Epoch 00017: loss improved from 2.71625 to 2.70675, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 18/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.7028\n",
            "\n",
            "Epoch 00018: loss improved from 2.70675 to 2.70276, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 19/100\n",
            "82268/82268 [==============================] - 32s 388us/step - loss: 2.6944\n",
            "\n",
            "Epoch 00019: loss improved from 2.70276 to 2.69438, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 20/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.6853\n",
            "\n",
            "Epoch 00020: loss improved from 2.69438 to 2.68533, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 21/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.6763\n",
            "\n",
            "Epoch 00021: loss improved from 2.68533 to 2.67625, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 22/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.6724\n",
            "\n",
            "Epoch 00022: loss improved from 2.67625 to 2.67235, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 23/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.6621\n",
            "\n",
            "Epoch 00023: loss improved from 2.67235 to 2.66211, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 24/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.6611\n",
            "\n",
            "Epoch 00024: loss improved from 2.66211 to 2.66108, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 25/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.6508\n",
            "\n",
            "Epoch 00025: loss improved from 2.66108 to 2.65083, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 26/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.6464\n",
            "\n",
            "Epoch 00026: loss improved from 2.65083 to 2.64640, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 27/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.6379\n",
            "\n",
            "Epoch 00027: loss improved from 2.64640 to 2.63794, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 28/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.6344\n",
            "\n",
            "Epoch 00028: loss improved from 2.63794 to 2.63436, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 29/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.6227\n",
            "\n",
            "Epoch 00029: loss improved from 2.63436 to 2.62273, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 30/100\n",
            "82268/82268 [==============================] - 32s 388us/step - loss: 2.6229\n",
            "\n",
            "Epoch 00030: loss did not improve from 2.62273\n",
            "Epoch 31/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.6101\n",
            "\n",
            "Epoch 00031: loss improved from 2.62273 to 2.61012, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 32/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.6010\n",
            "\n",
            "Epoch 00032: loss improved from 2.61012 to 2.60097, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 33/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.5900\n",
            "\n",
            "Epoch 00033: loss improved from 2.60097 to 2.59001, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 34/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.5831\n",
            "\n",
            "Epoch 00034: loss improved from 2.59001 to 2.58313, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 35/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.5787\n",
            "\n",
            "Epoch 00035: loss improved from 2.58313 to 2.57873, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 36/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.5685\n",
            "\n",
            "Epoch 00036: loss improved from 2.57873 to 2.56852, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 37/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.5591\n",
            "\n",
            "Epoch 00037: loss improved from 2.56852 to 2.55914, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 38/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.5559\n",
            "\n",
            "Epoch 00038: loss improved from 2.55914 to 2.55593, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 39/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.5449\n",
            "\n",
            "Epoch 00039: loss improved from 2.55593 to 2.54489, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 40/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.5348\n",
            "\n",
            "Epoch 00040: loss improved from 2.54489 to 2.53484, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 41/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.5240\n",
            "\n",
            "Epoch 00041: loss improved from 2.53484 to 2.52402, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 42/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.5199\n",
            "\n",
            "Epoch 00042: loss improved from 2.52402 to 2.51990, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 43/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.5135\n",
            "\n",
            "Epoch 00043: loss improved from 2.51990 to 2.51352, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 44/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.5038\n",
            "\n",
            "Epoch 00044: loss improved from 2.51352 to 2.50379, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 45/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4970\n",
            "\n",
            "Epoch 00045: loss improved from 2.50379 to 2.49696, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 46/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4940\n",
            "\n",
            "Epoch 00046: loss improved from 2.49696 to 2.49397, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 47/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4839\n",
            "\n",
            "Epoch 00047: loss improved from 2.49397 to 2.48392, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 48/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4780\n",
            "\n",
            "Epoch 00048: loss improved from 2.48392 to 2.47798, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 49/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4657\n",
            "\n",
            "Epoch 00049: loss improved from 2.47798 to 2.46572, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 50/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4582\n",
            "\n",
            "Epoch 00050: loss improved from 2.46572 to 2.45819, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 51/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4482\n",
            "\n",
            "Epoch 00051: loss improved from 2.45819 to 2.44819, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 52/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.4429\n",
            "\n",
            "Epoch 00052: loss improved from 2.44819 to 2.44287, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 53/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4380\n",
            "\n",
            "Epoch 00053: loss improved from 2.44287 to 2.43804, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 54/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4276\n",
            "\n",
            "Epoch 00054: loss improved from 2.43804 to 2.42757, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 55/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4153\n",
            "\n",
            "Epoch 00055: loss improved from 2.42757 to 2.41527, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 56/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4129\n",
            "\n",
            "Epoch 00056: loss improved from 2.41527 to 2.41286, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 57/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.4066\n",
            "\n",
            "Epoch 00057: loss improved from 2.41286 to 2.40657, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 58/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3977\n",
            "\n",
            "Epoch 00058: loss improved from 2.40657 to 2.39770, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 59/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3946\n",
            "\n",
            "Epoch 00059: loss improved from 2.39770 to 2.39459, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 60/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3860\n",
            "\n",
            "Epoch 00060: loss improved from 2.39459 to 2.38601, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 61/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.3800\n",
            "\n",
            "Epoch 00061: loss improved from 2.38601 to 2.38002, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 62/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3731\n",
            "\n",
            "Epoch 00062: loss improved from 2.38002 to 2.37309, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 63/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3639\n",
            "\n",
            "Epoch 00063: loss improved from 2.37309 to 2.36394, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 64/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3588\n",
            "\n",
            "Epoch 00064: loss improved from 2.36394 to 2.35883, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 65/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3536\n",
            "\n",
            "Epoch 00065: loss improved from 2.35883 to 2.35362, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 66/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3480\n",
            "\n",
            "Epoch 00066: loss improved from 2.35362 to 2.34798, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 67/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3409\n",
            "\n",
            "Epoch 00067: loss improved from 2.34798 to 2.34091, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 68/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3365\n",
            "\n",
            "Epoch 00068: loss improved from 2.34091 to 2.33648, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 69/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3286\n",
            "\n",
            "Epoch 00069: loss improved from 2.33648 to 2.32856, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 70/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3205\n",
            "\n",
            "Epoch 00070: loss improved from 2.32856 to 2.32047, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 71/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.3157\n",
            "\n",
            "Epoch 00071: loss improved from 2.32047 to 2.31570, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 72/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.3069\n",
            "\n",
            "Epoch 00072: loss improved from 2.31570 to 2.30695, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 73/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2971\n",
            "\n",
            "Epoch 00073: loss improved from 2.30695 to 2.29708, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 74/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2976\n",
            "\n",
            "Epoch 00074: loss did not improve from 2.29708\n",
            "Epoch 75/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2908\n",
            "\n",
            "Epoch 00075: loss improved from 2.29708 to 2.29076, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 76/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2858\n",
            "\n",
            "Epoch 00076: loss improved from 2.29076 to 2.28582, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 77/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2819\n",
            "\n",
            "Epoch 00077: loss improved from 2.28582 to 2.28188, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 78/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2752\n",
            "\n",
            "Epoch 00078: loss improved from 2.28188 to 2.27523, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 79/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2674\n",
            "\n",
            "Epoch 00079: loss improved from 2.27523 to 2.26745, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 80/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2619\n",
            "\n",
            "Epoch 00080: loss improved from 2.26745 to 2.26195, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 81/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.2591\n",
            "\n",
            "Epoch 00081: loss improved from 2.26195 to 2.25905, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 82/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2504\n",
            "\n",
            "Epoch 00082: loss improved from 2.25905 to 2.25040, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 83/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2451\n",
            "\n",
            "Epoch 00083: loss improved from 2.25040 to 2.24512, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 84/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2423\n",
            "\n",
            "Epoch 00084: loss improved from 2.24512 to 2.24229, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 85/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.2352\n",
            "\n",
            "Epoch 00085: loss improved from 2.24229 to 2.23517, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 86/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2294\n",
            "\n",
            "Epoch 00086: loss improved from 2.23517 to 2.22940, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 87/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2269\n",
            "\n",
            "Epoch 00087: loss improved from 2.22940 to 2.22685, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 88/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.2370\n",
            "\n",
            "Epoch 00088: loss did not improve from 2.22685\n",
            "Epoch 89/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.2199\n",
            "\n",
            "Epoch 00089: loss improved from 2.22685 to 2.21990, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 90/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.2099\n",
            "\n",
            "Epoch 00090: loss improved from 2.21990 to 2.20994, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 91/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.2055\n",
            "\n",
            "Epoch 00091: loss improved from 2.20994 to 2.20551, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 92/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.2013\n",
            "\n",
            "Epoch 00092: loss improved from 2.20551 to 2.20134, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 93/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.1975\n",
            "\n",
            "Epoch 00093: loss improved from 2.20134 to 2.19751, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 94/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.1922\n",
            "\n",
            "Epoch 00094: loss improved from 2.19751 to 2.19217, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 95/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.1914\n",
            "\n",
            "Epoch 00095: loss improved from 2.19217 to 2.19140, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 96/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.1795\n",
            "\n",
            "Epoch 00096: loss improved from 2.19140 to 2.17948, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 97/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.1755\n",
            "\n",
            "Epoch 00097: loss improved from 2.17948 to 2.17547, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 98/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.1750\n",
            "\n",
            "Epoch 00098: loss improved from 2.17547 to 2.17504, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 99/100\n",
            "82268/82268 [==============================] - 32s 389us/step - loss: 2.1729\n",
            "\n",
            "Epoch 00099: loss improved from 2.17504 to 2.17286, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n",
            "Epoch 100/100\n",
            "82268/82268 [==============================] - 32s 390us/step - loss: 2.1629\n",
            "\n",
            "Epoch 00100: loss improved from 2.17286 to 2.16290, saving model to Eip_Phase_2_ Assignment2-amit.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f998a632eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRd4RakZ8tPm",
        "colab_type": "code",
        "outputId": "566b6da6-5b4b-4261-e75a-d2982d6b216e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "import sys\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "for i in range(700):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" acups as the march hare and his friends\n",
            "shared their neverending meal and the shrill voice of the qu \"\n",
            "t and eroiet g ent  to tre of tr ani toe ennge and notereddl tu ani toe ennge and noteled ootele  tu enterriatid lotered ooteree toe er tr ani toe enn and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani toe ennge and notereddl tu ani t\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}